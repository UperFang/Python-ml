{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、RL简介  \n",
    "## review 两种机器学习类型 ：\n",
    "### 有监督学习和无监督学习属于预测型；强化学习属于决策型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预测**\n",
    "* 根据数据预测所需输出（有监督学习）\n",
    "* 生成数据实例（无监督学习）  \n",
    "\n",
    "**决策**  \n",
    "***会引起环境的改变***     \n",
    "* 在动态环境中采取行动（强化学习）\n",
    "转变到新的状态  \n",
    "获得即时奖励  \n",
    "随着时间推移**最大化累计奖励**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义  \n",
    "* 通过从交互中学习来实现目标的计算方法  \n",
    "* 在动态环境中去优化采取的行动使得随着时间推移最大化累计奖励的机器学习——强化学习\n",
    "![agent模型](.\\pic\\agent.png)\n",
    "* 三个方面：  \n",
    "1、 感知——环境   \n",
    "2、 行动——可以采取行动来改变状态、达到目标  \n",
    "3、 目标——随着时间推移最大化累计奖励  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL系统要素\n",
    "### 历史History(观察、行动、奖励的序列）  \n",
    "$$ H_t = O_1,R_1,A_1,O_2,R_2,A_2,...,O_{t-1},R_{t-1},A_{t-1},O_t,R_t$$  \n",
    "### 状态State——**确定下一步发生的事**  \n",
    " 是关于历史的函数$$ S_t = f(H_t) $$\n",
    "### 策略Policy Agent在特定状态s下的行为方式  \n",
    "1、确定性策略$$a=\\pi(s)$$  \n",
    "2、随机策略$$\\pi(a|s)=P(A_t = a|S_t = s)$$\n",
    "### 奖励Reward  \n",
    "1、一个维度——方便比较好坏（标量）\n",
    "### 价值函数Value Function  \n",
    "1、状态价值是一个标量，定义**长期**什么是好的  \n",
    "2、对未来累计奖励的预测  \n",
    "    用于评估在给定策略S下，状态的好坏\n",
    "$$ \\nu(s) = E_\\pi[R_{t+1}+{\\gamma}R_{t+2}+\\gamma^2R_{t+3}+...|S_t = s]（\\gamma \\in (0,1） $$\n",
    "Expectation over $\\pi$ 在给定策略下的期望\n",
    "### 环境Model\n",
    "1、预测下一个状态（一个条件概率）  \n",
    "2、预测下一个（**立即**）奖励  \n",
    "一个分布期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习智能体分类\n",
    "### 1、基于模型的强化学习\n",
    "1、整个环境都清楚，不需要交互  \n",
    "2、比如：迷宫、围棋  \n",
    "### 2、模型无关的强化学习（真正意义上的强化学习）\n",
    "1、没有环境模型，与环境交互  \n",
    "2、实时采样\n",
    "### 3、基于价值函数\n",
    "* 没有策略（隐含） \n",
    "* 确定价值函数  \n",
    "### 4、基于策略\n",
    "* 有一个策略$\\pi(s)$\n",
    "* 没有价值函数\n",
    "### 5、Actor-Critic\n",
    "* 策略\n",
    "* 价值函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、探索与利用\n",
    "由于当前决策和最优策略不等，所以需要**探索**——可能发现更好的策略  \n",
    "EX：选择一家晚餐吃完饭——常去喜欢的餐厅还是去一家没去的餐厅（探索）\n",
    "## 策略探索的一些原则、算法（没懂）\n",
    "* 朴素方法Naive Exploration——添加策略噪声\n",
    "* 积极初始化——随着探索feedback会下降，所以会导向没有探索过的，积极尝试新的？\n",
    "* 基于不确定性的度量——尝试具有不确定性收益的策略，可能带来更高的收益  \n",
    "* 概率匹配——基于概率选择最佳策略\n",
    "* 状态搜索——探索后续状态可能带来更高收益的策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多臂老虎机\n",
    "* **问题描述：**  \n",
    "1、动作集合：$$ a^i \\in A, i = 1,...,k$$  \n",
    "2、收益几何：$$ R(r|a^i) = P(r|a^i)$$\n",
    "K个老虎机，$a^i$表示选择第i个老虎机，收益$\\in(0,1)$  \n",
    "**目标：找到一个策略$\\pi$**  \n",
    "$$max\\sum_{t=1}^T{r_t},r_t\\sim{R(·|a)}$$\n",
    "**注意$r_t$是不确定的**  \n",
    "* 期望收益和采样次数的关系\n",
    "$$Q_n(a^i)=\\frac{r_1+r_2+...+r_{n-1}}{n-1}$$\n",
    "**缺点：每次更新的空间复杂度是O（n）**\n",
    "* 增量实现:\n",
    "$$Q_{n+1}(a^i)=\\frac 1n\\sum_{i=1}^n{r_i}=Q_n+\\frac 1n(r_n-Q_n)$$\n",
    "误差项$\\Delta_n^i=r_n-Q_n$  \n",
    "**空间复杂度为O（1）**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General算法\n",
    "* I. 初始化：$Q(a^i):=c^i,\\quad N(a^i)=0,i=1,...,n$  \n",
    "* II. 主循环$t=1:T$  \n",
    "    1、利用策略$\\pi$选取某个动作a\n",
    "    2、获取收益：$r_t=Bandit(a)$  \n",
    "    3、更新计数: $N(a):=N(a)+1$  \n",
    "    4、更新估值：$Q(a):=Q(a)+\\frac 1{N(a)}[r_t-Q(a)]$\n",
    "### e_greedy、UCB、Thompson Sampling算法\n",
    "![](.\\pic\\RLexplore-alo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫决策过程(Markov Decison Process, MDP)\n",
    "* 提供一套在**结果部分随机**、**部分在决策者控制下的决策过程建模**的数学框架  \n",
    "* 1、环境完全可观测  \n",
    "2、当前状态可以推演出接下来的情况（马尔科夫性质）  \n",
    "### **$S_{t+1}完全取决于S_t,不需要知道S_1，S_2...$**  \n",
    "### **含义**\n",
    "* 状态S从历史H中捕获了所有信息\n",
    "* 当S已指知时，可以不管H\n",
    "* 当前状态S是未来的充分统计量\n",
    "**整个Process由一个五元组表示(S,A,$P_{sa},\\gamma,R$)**\n",
    "* S——状态几何  \n",
    "* A——动作集合  \n",
    "* $P_{sa}$——在s,a下，下一个状态在S中的概率分布  \n",
    "* $\\gamma \\in [0,1]$——未来奖励的折扣因子\n",
    "* $R:S\\times A$奖励函数\n",
    "\n",
    "### MDP动态\n",
    "* ![MDPpro.png](.\\pic\\MDPpro.png)\n",
    "* ![MDPpro2.png](.\\pic\\MDPpro2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于动态规划的RL\n",
    "* ![基于动态规划的RL](.\\pic\\基于动态规划的RL.jpg)  \n",
    "* ![最优价值函数.png](.\\pic\\最优价值函数.png)  \n",
    "* 可以对**最优价值函数**和**最优策略**执行迭代更新  \n",
    "1、价值迭代  \n",
    "2、策略迭代  \n",
    "* ![价值迭代.png](.\\pic\\价值迭代.png)  \n",
    "* 同步价值迭代——存储两份内存  \n",
    "* 异步价值迭代——old和new在同一个位置 \n",
    "* ![策略迭代.png](.\\pic\\策略迭代.png)  \n",
    "* ![价值迭代和策略迭代总结.png](.\\pic\\价值迭代和策略迭代总结.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
